# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O5lc4w_sFK-Hf59TWdfFhomAqJeukDH5
"""

# Data
import numpy as np 
import pandas as pd 
import time 
import csv
from collections import Counter 
import matplotlib.pyplot as plt  
# Ontology
!pip install owlready2 
from owlready2 import * 
import re  # To separate words based on capital letters in onto classes & to split search queries
import sys
print("Import done")

def check_query(Enter_query, Enter_OntologyClasses, free_search):
  print("...Loading query...")
  parts = Enter_query.replace('| AND |','| OR |').split('| OR |')
  for part in parts:
    if part not in Enter_OntologyClasses and free_search == 'y':
      print(part + " not in ontology classes")
      Enter_OntologyClasses.append(part)
      print(part + " added to ontology classes")
    elif part not in Enter_OntologyClasses and free_search == 'n':
      print("WARNING: " + part +"is a non-ontological search terms while search is not set free. Set free_search to y, or change query.")

  print("..Query loaded...Start searching...")
  return(Enter_OntologyClasses)

def ImportAndMineData(onto_path, data_path, QUERY, free_search):
  # Import ontology
  try:
      onto = get_ontology(onto_path[0]).load()
  except:
      pass 
  onto = get_ontology(onto_path[0]).load()

  # Collecting classes from ontology in a list
  class_raw = list(get_ontology(onto_path[0]).load().classes())

  # select classes by Prefix, suffix, nametags
  tag = 'ElectrochemicalFlowCell'
  prefix = 'electrochemistry.'

  ClassCleaner = lambda x : re.sub('_',  ' ',\
                                   re.sub(r"(?<=\w)([A-Z])", r" \1", \
                                  str(x).removesuffix(tag).removeprefix(prefix))) \
                                      if (str(x).find(prefix) != -1) else "!EMPTY CLASS" 

  class_select = list(map(ClassCleaner, filter(lambda x : (str(x).find(tag) != -1), class_raw)))

  # Check if entered query contains ontology classes. 
    # If free search is on, ontology classes will be append, otherwise a warning wil be given. 
  class_select = check_query(QUERY, class_select)

  #import data
  df_raw = pd.read_csv(data_path) 

  print("Your imported ontology with "  + str(len(class_select)) + " classes is ready to use")

  """Data Process Functions"""

  # Function Text Cleaner
  f_CleanText = lambda text : re.split("\. |\! |\? ", text.lower()) if (type(text) == str)  else ["Not Available"]
      # Cleans text split scentices.
      # Output is of strings per for every text inputed. 
      # Example: Input = sentence1.sentence2!sentence3? --> Output = [sentence1,sentence2,sentence3]
    
    
  # Function Keyword Search    
  f_SearchKeyword = lambda text, keyword : text if (text.find(keyword) != -1) else None 
      # Searches text for keyword
     # Input: text = string or list of strings, keyword = string or list strings
     # Example: 
          # text = [sentence1,sentence2,sentence_A,sentence_B]
          # keyword = len(text)*[A]
          # Output  = [0,0,sentence_A,0]
        
  # Function Scan lists of lists with text
  f_ScanList = lambda text, keyword: list(map(f_SearchKeyword, text , len(text)*[keyword]))  
      # Example: 
          # text = [[sentence1,sentence2],[sentence_A,sentence_B]]
          # keyword = [A]
          # Output  = [[0,0],[sentence_A,0]]
                                        
  #f_ScanList = lambda text, keyword: f_SearchKeyword(text, 'keyword') #if (type(text) == str) else "appel"
  #f_ScanList = lambda text, keyword: len(text) if (type(text) == str) else "appel"

  """Process data"""

  # Function Keyword Search    
  f_SearchKeyword2 = lambda text, keyword : 'y' if (text.find(keyword) != -1) else 'n' 
      # Searches text for keyword
      # Input: text = string or list of strings, keyword = string or list strings
      # Example: 
          # text = [sentence1,sentence2,sentence_A,sentence_B]
          # keyword = len(text)*[A]
          # Output  = [n,n,y,n]
          # Function Scan lists of lists with text
  f_ScanList2 = lambda text, keyword: list(map(f_SearchKeyword2, text , len(text)*[keyword]))  
      # Example: 
          # text = [[sentence1,sentence2],[sentence_A,sentence_B]]
          # keyword = [A]
          # Output  = [[n,n],[y,n]]
        
  """Process Data"""
  start2 = time.time() # Measure time

  # New data frame to store processed data
  df_processed2 = df_raw.copy()
  SourceText = "Abstract Note" #Column name text source
  ProcessedText = "Clean Abstract Note" # Column name processed text

  # clean abstract data 
  df_processed2[ProcessedText] = df_raw[SourceText].map(f_CleanText)

  # function to search data for ontology classes !! depends on df_processed !!
  f_OntoSearch = lambda onto : list(map(f_ScanList2, df_processed2[ProcessedText], len(df_processed2[ProcessedText])*[onto.lower()]))

  # Search data ontology class and store in df
  def f_AddSearchResults(df, ontology):
      for onto in ontology: 
          df[str(onto)] = f_OntoSearch(onto)

  f_AddSearchResults(df_processed2, class_select)

  end2 = time.time() # Measure time
  calc_time = end2-start2  # Measure time

  print("Your " + str(len(df_processed2)) + " articles are searched on "  + str(len(class_select)) + " ontology classes in " + str(calc_time) + " seconds.")
  print("type df_processed to view data frame")

  """Import raw and processed data"""

  df_processed = df_processed2

  """Get Columns names of processed data to build the interface
          Note: to search the processed data, it is necessary to discriminate between the article
              specific classes and the processed data classes"""
    
  L_raw=list(df_raw.columns) # column names in raw data
  L_processed=list(df_processed.columns) # column names in processed data set
  # Type here below column names which are not onotlogy classes in the form ['name 2','name 2']
  L_processed_non_onto=['Unnamed: 0','Clean Abstract Note','!EMPTY CLASS'] 

  # Discriminate between column types
  article_specs = L_raw # Article specifics such as: DOI, Pub year, Title, ...
  ontology_classes = list((Counter(L_processed)-Counter(L_raw)-Counter(L_processed_non_onto)).elements()) # Ontology classes

  """Get text """
  #method1
  def get_text (text, code):
      #ans = ''
      l = []
      for i,char in enumerate(code):
          if char == 'y':
              l.append(text[i])
      #return ans
      return l

# #method2
# def get_text2 (df_text,df_code):
#     f_findtext = lambda df_texttext, df_codecode: df_texttext if (df_codecode=='y') else ''
#     f_txt = lambda df_text_row, df_code_row: list(map(f_findtext,df_text_row, df_code_row))
# #     f_txt = lambda df_text_row, df_code_row: [i for i in list(map(f_findtext,df_text_row, df_code_row)) if i is not None]
#     temp_ans = list(map(f_txt, df_text, df_code))
#     return temp_ans


  for onto in ontology_classes:
      temp_funct = lambda df_text, df_code: get_text(df_text, df_code)
      ans = list(map(temp_funct,list(df_processed["Clean Abstract Note"]), list(df_processed[onto])))
      df_processed[onto] = ans

  print("data colected")
  return(df_processed)

  def _calculateResult(search_query, data_frame):
        """Evaluate expressions."""
        # Functions to split and search query
        f_split_query = lambda query, separator : query.split(separator) # f_split_query(query = key1ANDkey2ORkey3) --> [[Key1,key2],[key3]]
        f_ANDsearch = lambda text, keyword1, keyword2 : "Ja" if (text.find(keyword1) != -1 and text.find(keyword2) != -1) else ''
        f_ORsearch = lambda text, keyword1, keyword2 : "Ja" if (text.find(keyword1) != -1 or text.find(keyword2) != -1) else ''
        
        # get data frame --> TODO: make this an input variable of _calculateResult
        df = data_frame # Data frame to search
        
        # ##########################
        # TO DO TO DO TO DO:
        # f_ANDsearch(df['keyword'],keyword,)
        
        # ##########################
        
        # Evaluate the expression entered in the display by the user ('=' is not displayed!)       
        que_screen = search_query  # get input que from screen
        que =  list(map(f_split_query,f_split_query(que_screen, '| OR |'), len(f_split_query(que_screen, '| OR |'))*["| AND |"])) # split que by 'and' & 'or' into ontoclasses
        dictio = {} # store (overlapping) numbers together with search terms
        dictio_searchResult = {} # Store search result to export
        
        answer = 'Search Results: '# Create string to store result
#        count = [None] * len(parts) # storage to count number of times an onto class is found
        for parts in que:  #Searches for articles in wich all ontology classes of 'parts' are found.
            L_parts = []
            n = -1
            overlap_total = []
            name = ''
            for part in parts:
                name = name + part + ' & '
                L_part = []
                n = n +1
                for row in range(0,len(df[part])): # Search row in df where onto is found in text
                    if len(df[part][row]) != 0:
                        L_part.append(row)
                L_parts.append(L_part)
        
                overlap = lambda a,b: set(a) & set(b)
                if n == 0:
                    overlap_total = L_parts[0]
                else:
                    overlap_total = overlap(overlap_total,L_parts[n])
            dictio[str(parts)] = list(overlap_total)
            dictio[name] = list(overlap_total)
            
        
        # !NOTE: This loop can not be included in the previous loop!
        for parts in que:
            dictio_searchResult[parts] = []
            for row in dictio[str(parts)]:
                answer = answer + "\n   " + str(parts) + " found in the abstract of DOI:" + str(df["DOI"][row]) + "  in the sentence(s):"
                dictio_searchResult[parts] = dictio_searchResult[parts].append(df["DOI"][row])
                for onto in parts:
                    answer = answer + "\n           " + str(df[onto][row])
        
            
#    dictio[str(que[0])]     
#         for part in parts: # if part is not a class in the onto, an error arise!
# #           storage = {}  --> Here we can store the result per part, so that we can add the logical opporators to the answers
# #            storage.append()
#             for row in range(0,len(df)):  # Select rows to enter values of onto class
#                 try: # try if search term is in database
#                     if type(df[part].iloc[row]) == list: # Check if Onto class has a value (i.e. if it is not None)
#                         if type(df["DOI"].iloc[row]) == str and df[part].iloc[row][0] == 'yes':
#     #                        count = count + 1
#                             answer = answer + "\n   " + part + " found in the abstract of DOI:" + df["DOI"].iloc[row] + "  in the sentence(s)"
#                             for sentence in range(1,len(df[part].iloc[row])):
#                                 answer = answer + "\n           " + df[part].iloc[row][sentence] # store DOI of article found
#                 except KeyError: # if search term is not in database no search can be done
#                     answer = part + ' Not found in data base'

    
#         df = df_processed.copy()
#         que = [["Pump","Electrolyte","Membrane"],["Electrolyte"]] #testQue

#         # Get overlap: get the articles where a word or a set of words is found.
#         dictio = {} # store (overlapping) numbers together with search terms


    
    
        ###COUNtCOUNTCOUNTCOUNTCOUNT!!!
        # Add "ONTO class FOUNd X times in database" 
        # Add "class1 AND clss 2 found C times in data base" etc.
                    
        # Show anwser 
        result = answer
        return(answer, dictio)
        #print(answer)
        # self._view.setDisplayText(result) # disabled to see input query on display
        #self._view.setOutputScreenText(result.split("\n"))

